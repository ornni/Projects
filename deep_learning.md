딥러닝=심층신경망(Deep Neural Network, DNN)<br>

뉴런으로 구성된 레이어를 여러 개 연결해서 구성한 네트워크

---

딥러닝의 탄생<br>

딥러닝(=신경망): 인간의 뇌가 생각하는 방식을 머신러닝알고리즘으로 설계한 것<br>

뉴런이 가지고 있는 정보는 가중치와 곱해져서 다음 레이어의 뉴런으로 전파<br>

![deep_learning](https://github.com/ornni/ML_algorithm/blob/main/image/2-1.png?raw=true)

---

딥러닝과 머신러닝의 관계<br>

![relation](https://github.com/ornni/ML_algorithm/blob/main/image/2-2.png?raw=true)

---

딥러닝<br>
![deep_learning_png](https://github.com/ornni/ML_algorithm/blob/main/image/3-1.png?raw=true)

---

손실함수<br>

출력값과 정답의 차이

---

최적화(optimization)<br>

매개변수를 조절해서 손실함수의 값을 최저로 만드는 과정

---

옵티마이저(optimizer)<br>

최적화의 과정은 옵티마이저를 통해 이루어짐

#

**배치 경사하강법**<br>

무작위로 부여된 매개변수에서부터 가장 가까운 로컬 미니멈에 멈추게 됨

#

**SGD(stochasticgradientdescent)**<br>

배치 경사하강법의 단점으로 고안된 방법<br>

하나의 데이터마다 매개변수를 변경하는 방법<br>

손실함수가 매우 불규칙하고 로컬 미니멈이 많을 때는 사용하기 좋은 최적화 알고리즘<br>

장점: 제한된 자원으로도 충분히 딥러닝 모델을 학습<br>

단점: 경사하강법보다 못한 매개변수로 학습될 수도 있음

#

**미니 배치**<br>

배치 경사하강법과SGD의 절충안<br>

전체 데이터를 계산해서 매개변수를 변경하는 대신 정해진 양만큼 계산해서 매개변수를 최적화하는 방법<br>

주기(epoch): 학습을 위해 전체 데이터를 다 사용했을 때 한 epoch가 지났다고 표현<br>

배치 사이즈: 매개변수 조정을 위해 한 번에 처리하는 데이터의 양<br>

스텝(step, iteration): 미니배치를 사용해 매개변수가 조정되는 순간마다 스텝이라고 함<br>

#

**모멘텀**<br>

최초 로컬 미니멈에 머무르지 않고, 더 낮은 로컬 미니멈까지 도달할 수 있다<br>

장점: 가장 가까운 로컬 미니멈에 머물지 않고, 더 나은 로컬 미니멈으로 모델을 최적화할 수 있음<br>

단점: 글로벌 미니멈으로 최적화된다는 보장 없음

#

**학습률**

#

**Adagrad**<br>

각 매개변수에 각기 다른 학습률을 적용하고, 빈번한 변화의 가중치는 작은 학습률, 변화가 적은 가중치의 학습률은 높게 설정되는 옵티마이저<br>

자연어 처리에서 장점이 확연히 드러남(one-hot encoding에서 유용)

#

**Adam**<br>

Adagrad의 학습률자율조정과 모멘텀의 효율적인 매개변수 변경 알고리즘을 조합한 알고리즘

---
딥러닝 과대적합 방지<br>

**드롭아웃(Dropout)**<br>

매 스텝마다 몇 개의 노드는 사용하지 않고 학습, 즉 매개변수 중 일정량을 학습 중간마다 무작위로 사용하지 않는 방법<br>

매개변수 조절, 분산을 줄이는 데 효과적

#

**조기종료(Early Stopping)**<br>

학습을 진행할 때 최대 반복 횟수를 설정하되, 모델이 과대적합의 소지가 있는 경우 학습을 중단하고 학습된 모델 중 최고의 모델 선택<br>

검증 데이터가 꾸준히 떨어지는 시점이 발견되면 즉시 학습을 중단하고, 지금까지 학습된 모델 중 최고의 검증 정확도를 갖는 모델로 테스트 진행
